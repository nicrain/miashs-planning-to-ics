"""
è¯¾ç¨‹è®¡åˆ’è§£æå™¨ - è‡ªåŠ¨ä»Google Sheetsç”ŸæˆICSæ—¥å†æ–‡ä»¶

åŠŸèƒ½ç‰¹æ€§:
- åŠ¨æ€æå–æœˆä»½URL
- è‡ªåŠ¨æ£€æµ‹åˆ é™¤çº¿è¯¾ç¨‹(CSSç±»å’Œå†…è”æ ·å¼)  
- æ”¯æŒé…ç½®æ–‡ä»¶æ‰‹åŠ¨å–æ¶ˆ
- ç”Ÿæˆæ ‡å‡†ICSæ ¼å¼æ—¥å†æ–‡ä»¶
- æ™ºèƒ½ç¼“å­˜æœºåˆ¶
- ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ

ä½œè€…: Assistant
ç‰ˆæœ¬: 5.1 (ä¼˜åŒ–ç‰ˆ)
æ—¥æœŸ: 2025-10-06
"""

import requests
import csv
import re
from ics import Calendar, Event
from datetime import datetime
import os
from zoneinfo import ZoneInfo
from io import StringIO
from bs4 import BeautifulSoup
from typing import Set, List, Dict, Tuple, Optional, Union, NamedTuple, ClassVar
import logging
from dataclasses import dataclass
from functools import lru_cache
import hashlib
from urllib.parse import urlparse
import time
from contextlib import contextmanager

# é…ç½®å¸¸é‡
class Config:
    # æ ¸å¿ƒURLå’Œæ–‡ä»¶é…ç½®
    PLANNING_URL = "https://handiman.univ-paris8.fr/planningM2-25-26.php"
    OUTPUT_FILENAME = "master_handi_schedule.ics"
    CONFIG_FILE = "cancelled_dates.txt"
    TIMEZONE = "Europe/Paris"
    
    # ç½‘ç»œè¯·æ±‚é…ç½®
    REQUEST_TIMEOUT = 20
    MAX_RETRIES = 3
    CACHE_SIZE = 128
    
    # HTMLæ£€æµ‹é…ç½®
    STRIKETHROUGH_PATTERNS: ClassVar[List[str]] = [
        r'text-decoration:\s*line-through',
        r'text-decoration-line:\s*line-through',
    ]
    
    # æ—¶é—´è§£æé…ç½®
    # æ”¯æŒ h å’Œ : ä½œä¸ºåˆ†éš”ç¬¦
    TIME_PATTERN = r'(\d{1,2}(?:h|:)(?:\d{2})?)\s*-\s*(\d{1,2}(?:h|:)(?:\d{2})?)'
    DATE_PATTERN = r'(\d{1,2})/(\d{1,2})(?:/(\d{4}))?'
    
    # æ”¯æŒçš„æœˆä»½åç§°ï¼ˆå«å­—ç¬¦æ ‡å‡†åŒ–æ˜ å°„ï¼‰
    SUPPORTED_MONTHS = {
        'septembre', 'octobre', 'novembre', 'decembre', 'janvier', 'fevrier',
        'mars', 'avril', 'mai', 'juin', 'juillet'
    }
    
    MONTH_NORMALIZATION = {
        'Ã©': 'e', 'Ã¨': 'e', 'Ã§': 'c', 'Ã ': 'a', 'Ã¹': 'u', 'Ã´': 'o'
    }
    
    # Webcalè®¢é˜…æ›´æ–°å‘¨æœŸé…ç½®
    ENABLE_REFRESH_INTERVAL = True
    REFRESH_INTERVAL_HOURS = 1  # æ¯å°æ—¶æ£€æŸ¥æ›´æ–°
    PUBLISH_TTL_HOURS = 1       # 1å°æ—¶ç¼“å­˜TTL
    CALENDAR_METHOD = "PUBLISH" # å‘å¸ƒæ¨¡å¼
    
    # æ ‡å‡†è¯·æ±‚å¤´
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

@dataclass(frozen=True)
class CancelledEvent:
    """å–æ¶ˆçš„äº‹ä»¶ä¿¡æ¯"""
    date: Tuple[int, int, int]  # (year, month, day)
    content: str
    event_type: str  # 'full' æˆ– 'partial'

class ParsedTime(NamedTuple):
    """è§£æçš„æ—¶é—´ä¿¡æ¯"""
    hour: int
    minute: int

@contextmanager
def timer(operation: str):
    """æ€§èƒ½è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = time.time()
    try:
        yield
    finally:
        elapsed = time.time() - start_time
        logger.info(f"â±ï¸ {operation} è€—æ—¶: {elapsed:.2f}ç§’")

# é…ç½®ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(levelname)s: %(message)s',
        datefmt='%H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

@lru_cache(maxsize=Config.CACHE_SIZE)
def extract_month_urls(planning_url: str = None) -> Optional[Dict[str, str]]:
    """
    ä»ä¸»é¡µé¢æå–æ‰€æœ‰æœˆä»½å’Œå¯¹åº”çš„Google Sheets URL (å¸¦ç¼“å­˜)
    
    Args:
        planning_url: ä¸»é¡µé¢URLï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„URL
        
    Returns:
        Dict[str, str]: æœˆä»½åç§°åˆ°CSV URLçš„æ˜ å°„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    if planning_url is None:
        planning_url = Config.PLANNING_URL
        
    logger.info("æ­£åœ¨ä»ä¸»é¡µé¢æå–æœˆä»½ä¿¡æ¯...")
    
    for attempt in range(Config.MAX_RETRIES):
        try:
            response = requests.get(
                planning_url, 
                headers=Config.HEADERS, 
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            html_content = response.text
            
            # æå–iframeæºé“¾æ¥
            iframe_pattern = r'<iframe[^>]*src=[\'"]([^\'">]*pubhtml[^\'">]*)[\'"][^>]*>'
            matches = re.findall(iframe_pattern, html_content)
            
            # æå–æœˆä»½æ ‡ç­¾
            label_pattern = r'<label[^>]*>([^<]+)</label>'
            month_labels = re.findall(label_pattern, html_content)
            
            # è¿‡æ»¤å’Œæ ‡å‡†åŒ–æœˆä»½åç§°
            month_names = []
            for label in month_labels:
                # ä½¿ç”¨é…ç½®ä¸­çš„æ ‡å‡†åŒ–æ˜ å°„
                normalized_month = label.lower()
                for old_char, new_char in Config.MONTH_NORMALIZATION.items():
                    normalized_month = normalized_month.replace(old_char, new_char)
                    
                if normalized_month in Config.SUPPORTED_MONTHS:
                    month_names.append(normalized_month)
            
            if len(matches) == len(month_names):
                urls = {}
                for month, html_url in zip(month_names, matches):
                    # è½¬æ¢HTML URLä¸ºCSV URL
                    csv_url = html_url.replace('/pubhtml?', '/pub?') + '&output=csv'
                    urls[month] = csv_url
                    logger.info(f"æ‰¾åˆ°: {month} -> {csv_url[:60]}...")
                
                return urls
            else:
                logger.warning(f"æœˆä»½æ•°é‡({len(month_names)})ä¸é“¾æ¥æ•°é‡({len(matches)})ä¸åŒ¹é…")
                return None
                
        except requests.RequestException as e:
            if attempt < Config.MAX_RETRIES - 1:
                logger.warning(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{Config.MAX_RETRIES}): {e}")
                continue
            else:
                logger.error(f"ç½‘ç»œè¯·æ±‚æœ€ç»ˆå¤±è´¥: {e}")
                return None
        except Exception as e:
            logger.error(f"æå–æœˆä»½ä¿¡æ¯å¤±è´¥: {e}")
            return None

def parse_date_string(date_str: str) -> Optional[Tuple[int, int, int]]:
    """
    è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸º(year, month, day)å…ƒç»„
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒæ ¼å¼ DD/MM æˆ– DD/MM/YYYY
        
    Returns:
        Tuple[int, int, int]: (year, month, day)ï¼Œè§£æå¤±è´¥è¿”å›None
    """
    date_match = re.match(Config.DATE_PATTERN, date_str.strip())
    if date_match:
        day = int(date_match.group(1))
        month = int(date_match.group(2))
        # 1æœˆåˆ°7æœˆå±äº2026å¹´ï¼ˆæ˜¥å­£å­¦æœŸï¼‰ï¼Œ8æœˆåˆ°12æœˆå±äº2025å¹´ï¼ˆç§‹å­£å­¦æœŸï¼‰
        year = int(date_match.group(3)) if date_match.group(3) else (2026 if 1 <= month <= 7 else 2025)
        
        # åŸºæœ¬æ—¥æœŸéªŒè¯
        if not (1 <= day <= 31 and 1 <= month <= 12):
            logger.warning(f"æ— æ•ˆæ—¥æœŸ: {day:02d}/{month:02d}/{year}")
            return None
            
        return (year, month, day)
    return None

def load_cancelled_dates(config_file: str = None) -> Set[Tuple[int, int, int]]:
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½è¢«å–æ¶ˆçš„æ—¥æœŸ
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨Config.CONFIG_FILE
        
    Returns:
        Set[Tuple[int, int, int]]: è¢«å–æ¶ˆæ—¥æœŸçš„é›†åˆ (year, month, day)
    """
    if config_file is None:
        config_file = Config.CONFIG_FILE
        
    cancelled_dates = set()
    
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        parsed_date = parse_date_string(line)
                        if parsed_date:
                            year, month, day = parsed_date
                            cancelled_dates.add((year, month, day))
                            logger.info(f"é…ç½®: å°†è·³è¿‡ {day:02d}/{month:02d}/{year} çš„è¯¾ç¨‹")
                        else:
                            logger.warning(f"é…ç½®æ–‡ä»¶ç¬¬{line_num}è¡Œæ—¥æœŸæ ¼å¼é”™è¯¯: {line}")
        else:
            logger.info(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {config_file}ï¼Œä¸è·³è¿‡ä»»ä½•æ—¥æœŸ")
    
    except FileNotFoundError:
        logger.info(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä¸è·³è¿‡ä»»ä½•æ—¥æœŸ")
    except PermissionError:
        logger.error(f"æ²¡æœ‰æƒé™è¯»å–é…ç½®æ–‡ä»¶ {config_file}")
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    return cancelled_dates

def detect_strikethrough_from_html(csv_url: str) -> Tuple[Set[Tuple[int, int, int]], List[CancelledEvent]]:
    """é€šè¿‡ä¼˜åŒ–çš„HTMLè®¿é—®å‚æ•°æ£€æµ‹åˆ é™¤çº¿æ ·å¼çš„æ—¥æœŸå’Œäº‹ä»¶"""
    cancelled_dates: Set[Tuple[int, int, int]] = set()
    cancelled_events: List[CancelledEvent] = []
    
    logger.info("ğŸ” æ­£åœ¨å°è¯•HTMLåˆ é™¤çº¿æ£€æµ‹...")
    
    try:
        # æ„å»ºèƒ½è·å–å®Œæ•´æ ·å¼çš„HTML URL (å…³é”®ï¼šwidget=false)
        base_url = csv_url.replace('/pub?', '/pubhtml?').replace('&output=csv', '')
        
        # ä½¿ç”¨å‘ç°çš„æœ‰æ•ˆå‚æ•°ç»„åˆ
        html_urls = [
            base_url + '&widget=false',  # å…³é”®å‚æ•°ï¼èƒ½è·å–å®Œæ•´æ ·å¼
            base_url,  # å¤‡ç”¨ï¼šåŸºç¡€URL
        ]
        
        for i, html_url in enumerate(html_urls):
            try:
                logger.debug(f"å°è¯•URLå˜ä½“ {i+1}: ...{html_url[-50:]}")
                response = requests.get(html_url, headers=Config.HEADERS, timeout=Config.REQUEST_TIMEOUT)
                response.raise_for_status()
                
                # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æ ·å¼ä¿¡æ¯
                html_content = response.text
                if any(pattern for pattern in Config.STRIKETHROUGH_PATTERNS 
                      if re.search(pattern, html_content)):
                    logger.info("âœ“ æ‰¾åˆ°åŒ…å«æ ·å¼çš„HTMLå†…å®¹!")
                    cancelled_dates, cancelled_events = parse_html_for_strikethrough(html_content)
                    if cancelled_dates or cancelled_events:
                        return cancelled_dates, cancelled_events
                else:
                    logger.debug("è¯¥URLæœªåŒ…å«å®Œæ•´æ ·å¼ä¿¡æ¯")
                    
            except requests.RequestException as e:
                logger.warning(f"URLå˜ä½“ {i+1} è¯·æ±‚å¤±è´¥: {e}")
                continue
            except Exception as e:
                logger.warning(f"URLå˜ä½“ {i+1} å¤„ç†å¤±è´¥: {e}")
                continue
        
        logger.info("æ‰€æœ‰HTMLè®¿é—®å°è¯•å‡æœªæˆåŠŸè·å–æ ·å¼ä¿¡æ¯")
        
    except Exception as e:
        logger.error(f"HTMLæ£€æµ‹æ•´ä½“å¤±è´¥: {e}")
    
    return cancelled_dates, cancelled_events

def parse_html_for_strikethrough(html_source: Union[str, os.PathLike]) -> Tuple[Set[Tuple[int, int, int]], List[CancelledEvent]]:
    """
    è§£æHTMLå†…å®¹æŸ¥æ‰¾åˆ é™¤çº¿æ ·å¼çš„æ—¥æœŸå’Œå…·ä½“è¯¾ç¨‹å†…å®¹
    
    Args:
        html_source: HTMLå†…å®¹å­—ç¬¦ä¸²æˆ–HTMLæ–‡ä»¶è·¯å¾„
        
    Returns:
        Tuple[Set, List]: (å®Œå…¨å–æ¶ˆçš„æ—¥æœŸé›†åˆ, éƒ¨åˆ†å–æ¶ˆçš„äº‹ä»¶åˆ—è¡¨)
    """
    cancelled_dates = set()
    cancelled_events = []
    
    try:
        # æ™ºèƒ½æ£€æµ‹è¾“å…¥ç±»å‹ï¼šæ–‡ä»¶è·¯å¾„è¿˜æ˜¯HTMLå†…å®¹
        if isinstance(html_source, (str, os.PathLike)) and len(str(html_source)) < 300 and str(html_source).endswith('.html'):
            with open(html_source, 'r', encoding='utf-8') as f:
                html_content = f.read()
        else:
            html_content = str(html_source)
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æ–¹æ³•1: æŸ¥æ‰¾CSSç±»çº§åˆ«çš„åˆ é™¤çº¿æ ·å¼ï¼ˆæ•´ä¸ªå•å…ƒæ ¼å–æ¶ˆï¼‰
        strikethrough_classes = set()
        style_tags = soup.find_all('style')
        
        for style_tag in style_tags:
            css_content = style_tag.string or ''
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å¼
            for pattern in Config.STRIKETHROUGH_PATTERNS:
                css_rules = re.findall(rf'\.([a-zA-Z0-9_-]+)\{{[^}}]*{pattern}[^}}]*\}}', css_content)
                strikethrough_classes.update(css_rules)
                if css_rules:
                    logger.info(f"âœ“ æ‰¾åˆ°CSSåˆ é™¤çº¿æ ·å¼ç±»: {css_rules}")
        
        # å¤„ç†CSSç±»çº§åˆ«çš„åˆ é™¤çº¿
        if strikethrough_classes:
            for class_name in strikethrough_classes:
                cells = soup.find_all('td', class_=lambda x: x and class_name in x.split())
                logger.info(f"æ‰¾åˆ° {len(cells)} ä¸ªä½¿ç”¨ {class_name} ç±»çš„å•å…ƒæ ¼")
                
                for cell in cells:
                    current_row = cell.find_parent('tr')
                    if current_row:
                        current_cells = current_row.find_all('td')
                        cell_position = -1
                        for j, curr_cell in enumerate(current_cells):
                            if curr_cell == cell:
                                cell_position = j
                                break
                        
                        prev_row = current_row.find_previous_sibling('tr')
                        if prev_row and cell_position >= 0:
                            prev_cells = prev_row.find_all('td')
                            if cell_position < len(prev_cells):
                                date_cell = prev_cells[cell_position]
                                date_text = date_cell.get_text(strip=True)
                                
                                date_matches = re.findall(r'\b(\d{1,2})/(\d{1,2})\b', date_text)
                                for day_str, month_str in date_matches:
                                    day, month = int(day_str), int(month_str)
                                    # 1æœˆåˆ°7æœˆå±äº2026å¹´ï¼ˆæ˜¥å­£å­¦æœŸï¼‰ï¼Œ8æœˆåˆ°12æœˆå±äº2025å¹´ï¼ˆç§‹å­£å­¦æœŸï¼‰
                                    year = 2026 if 1 <= month <= 7 else 2025
                                    cancelled_dates.add((year, month, day))
                                    logger.info(f"ğŸš« æ£€æµ‹åˆ°æ•´æ—¥è¯¾ç¨‹å–æ¶ˆ: {day:02d}/{month:02d}/{year}")
        
        # æ–¹æ³•2: æŸ¥æ‰¾å†…è”æ ·å¼çš„åˆ é™¤çº¿ï¼ˆspançº§åˆ«çš„éƒ¨åˆ†å–æ¶ˆï¼‰
        inline_strikethrough_spans = soup.find_all('span', 
            style=lambda x: x and any(re.search(pattern, x) for pattern in Config.STRIKETHROUGH_PATTERNS))
        logger.info(f"æ‰¾åˆ° {len(inline_strikethrough_spans)} ä¸ªå†…è”åˆ é™¤çº¿spanå…ƒç´ ")
        
        for span in inline_strikethrough_spans:
            parent_cell = span.find_parent('td')
            if parent_cell:
                current_row = parent_cell.find_parent('tr')
                if current_row:
                    current_cells = current_row.find_all('td')
                    cell_position = -1
                    for j, curr_cell in enumerate(current_cells):
                        if curr_cell == parent_cell:
                            cell_position = j
                            break
                    
                    prev_row = current_row.find_previous_sibling('tr')
                    if prev_row and cell_position >= 0:
                        prev_cells = prev_row.find_all('td')
                        if cell_position < len(prev_cells):
                            date_cell = prev_cells[cell_position]
                            date_text = date_cell.get_text(strip=True)
                            
                            date_matches = re.findall(r'\b(\d{1,2})/(\d{1,2})\b', date_text)
                            for day_str, month_str in date_matches:
                                day, month = int(day_str), int(month_str)
                                # 1æœˆåˆ°7æœˆå±äº2026å¹´ï¼ˆæ˜¥å­£å­¦æœŸï¼‰ï¼Œ8æœˆåˆ°12æœˆå±äº2025å¹´ï¼ˆç§‹å­£å­¦æœŸï¼‰
                                year = 2026 if 1 <= month <= 7 else 2025
                                
                                cancelled_content = span.get_text(strip=True)
                                cancelled_events.append(CancelledEvent(
                                    date=(year, month, day),
                                    content=cancelled_content,
                                    event_type='partial'
                                ))
                                logger.info(f"ğŸš« æ£€æµ‹åˆ°éƒ¨åˆ†è¯¾ç¨‹å–æ¶ˆ: {day:02d}/{month:02d}/{year}")
                                logger.info(f"    å–æ¶ˆå†…å®¹: {cancelled_content}")
        
    except Exception as e:
        logger.error(f"HTMLè§£æå¤±è´¥: {e}")
    
    return cancelled_dates, cancelled_events

def parse_time(time_str: str) -> Optional[ParsedTime]:
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œè¿”å›ParsedTimeå¯¹è±¡"""
    # ç»Ÿä¸€æ ¼å¼ï¼šå°† : æ›¿æ¢ä¸º hï¼Œæ–¹ä¾¿å¤„ç†
    time_str = time_str.lower().replace(':', 'h')
    
    if 'h' in time_str:
        parts = time_str.split('h')
        try:
            hour = int(parts[0])
            minute = int(parts[1]) if len(parts) > 1 and parts[1] else 0
            return ParsedTime(hour=hour, minute=minute)
        except (ValueError, IndexError):
            logger.warning(f"æ— æ³•è§£ææ—¶é—´å­—ç¬¦ä¸²: {time_str}")
    return None

def parse_cell_content(cell_text: str, year: int, month: int, day: int, 
                      cancelled_events: Optional[List[CancelledEvent]] = None) -> List[Event]:
    """è§£æå•å…ƒæ ¼å†…å®¹ï¼Œæå–è¯¾ç¨‹äº‹ä»¶ï¼Œå¹¶æ’æŸ¥éƒ¨åˆ†å–æ¶ˆçš„äº‹ä»¶"""
    events: List[Event] = []
    if not cell_text.strip():
        return events
    
    # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†å–æ¶ˆçš„äº‹ä»¶
    cancelled_content_list = []
    if cancelled_events:
        for cancelled_event in cancelled_events:
            if cancelled_event.date == (year, month, day):
                cancelled_content_list.append(cancelled_event.content.strip())
    
    logger.debug(f"æ—¥æœŸ {day:02d}/{month:02d}/{year} çš„å–æ¶ˆå†…å®¹: {cancelled_content_list}")
    
    # æŒ‰ç©ºè¡Œåˆ†å‰²ä¸åŒçš„äº‹ä»¶å—
    event_blocks = re.split(r'\n\s*\n', cell_text.strip())
    
    for block in event_blocks:
        lines = [line.strip() for line in block.split('\n') if line.strip()]
        if not lines:
            continue
            
        # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´ä¿¡æ¯ï¼ˆæ”¹è¿›ï¼šæ‰¾å‡ºæ‰€æœ‰æ—¶é—´æ®µï¼Œè€Œä¸åªæ˜¯ç¬¬ä¸€ä¸ªï¼‰
        time_events = []
        main_instructor = ""
        global_description_lines = []
        
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰æ—¶é—´åŒ¹é…å’ŒåŸºæœ¬ä¿¡æ¯
        for i, line in enumerate(lines):
            # åŒ¹é…æ—¶é—´æ ¼å¼ï¼š9h-12h, 14h30-17h30, 18h-21h ç­‰
            time_matches = re.finditer(Config.TIME_PATTERN, line)
            for time_match in time_matches:
                start_str, end_str = time_match.groups()
                
                # æå–è¯¾ç¨‹åç§°ï¼ˆé€šå¸¸åœ¨æ—¶é—´åé¢ï¼‰
                time_end = time_match.end()
                remaining_text = line[time_end:].strip()
                if remaining_text.startswith(':'):
                    remaining_text = remaining_text[1:].strip()
                
                event_name = remaining_text if remaining_text else "Ã‰vÃ©nement"
                
                time_events.append({
                    'line': line,
                    'start_str': start_str,
                    'end_str': end_str,
                    'event_name': event_name,
                    'original_line_index': i
                })
            
            # æŸ¥æ‰¾æ•™å¸ˆåç§°ï¼ˆé€šå¸¸æ˜¯å§“åæ ¼å¼ï¼‰
            line_text = line.strip()
            if re.match(r'^[A-Z][a-z]+\s+[A-Z][a-z]+', line_text) or 'Mohammed' in line_text or 'Marie' in line_text:
                if not main_instructor:  # åªå–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ•™å¸ˆåç§°ä½œä¸ºä¸»æ•™å¸ˆ
                    main_instructor = line_text
            elif not any(Config.TIME_PATTERN in line for pattern in [Config.TIME_PATTERN] if re.search(pattern, line)):
                # å¦‚æœè¿™è¡Œä¸åŒ…å«æ—¶é—´ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯æè¿°
                global_description_lines.append(line_text)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ—¶é—´äº‹ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤äº‹ä»¶
        if not time_events:
            time_line = None
            event_name = "Ã‰vÃ©nement"
            instructor = ""
            description_lines = []
            
            for i, line in enumerate(lines):
                # åŒ¹é…æ—¶é—´æ ¼å¼ï¼š9h-12h, 14h30-17h30, 18h-21h ç­‰
                time_match = re.search(Config.TIME_PATTERN, line)
                if time_match:
                    time_line = line
                    start_str, end_str = time_match.groups()
                    
                    # æå–è¯¾ç¨‹åç§°ï¼ˆé€šå¸¸åœ¨æ—¶é—´åé¢ï¼‰
                    time_end = time_match.end()
                    remaining_text = line[time_end:].strip()
                    if remaining_text.startswith(':'):
                        remaining_text = remaining_text[1:].strip()
                    
                    if remaining_text:
                        event_name = remaining_text
                    elif i + 1 < len(lines):
                        event_name = lines[i + 1]
                    
                    # æŸ¥æ‰¾æ•™å¸ˆåç§°ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€è¡Œæˆ–ç‰¹å®šæ ¼å¼ï¼‰
                    for j in range(i + 1, len(lines)):
                        line_text = lines[j].strip()
                        # æ•™å¸ˆåç§°é€šå¸¸æ˜¯å§“åæ ¼å¼
                        if re.match(r'^[A-Z][a-z]+\s+[A-Z][a-z]+', line_text) or 'Mohammed' in line_text or 'Marie' in line_text:
                            instructor = line_text
                        else:
                            description_lines.append(line_text)
                    
                    break
        
        # å¤„ç†æ‰¾åˆ°çš„æ—¶é—´äº‹ä»¶
        if time_events:
            # ä¸ºæ¯ä¸ªæ—¶é—´æ®µåˆ›å»ºç‹¬ç«‹çš„äº‹ä»¶
            for time_event in time_events:
                time_line = time_event['line']
                start_str = time_event['start_str']
                end_str = time_event['end_str']
                event_name = time_event['event_name']
                
                # æ£€æŸ¥è¿™ä¸ªäº‹ä»¶æ˜¯å¦è¢«éƒ¨åˆ†å–æ¶ˆ
                event_cancelled = False
                for cancelled_content in cancelled_content_list:
                    if cancelled_content and (
                        cancelled_content in block or 
                        any(cancelled_content in line for line in lines) or
                        f"{start_str}-{end_str}" in cancelled_content
                    ):
                        logger.info(f"âš ï¸  è·³è¿‡è¢«å–æ¶ˆçš„äº‹ä»¶: {time_line[:50]}...")
                        event_cancelled = True
                        break
                
                if event_cancelled:
                    continue
                    
                try:
                    start_time = parse_time(start_str)
                    end_time = parse_time(end_str)
                    
                    if start_time and end_time:
                        event = Event()
                        # ä½¿ç”¨å·´é»æ—¶åŒº
                        paris_tz = ZoneInfo(Config.TIMEZONE)
                        event.begin = datetime(year, month, day, start_time.hour, start_time.minute, tzinfo=paris_tz)
                        event.end = datetime(year, month, day, end_time.hour, end_time.minute, tzinfo=paris_tz)
                        
                        # æ„å»ºäº‹ä»¶æ ‡é¢˜
                        final_event_name = event_name
                        
                        # å¦‚æœäº‹ä»¶åç§°æ˜¯é»˜è®¤å€¼ï¼Œä¸”æœ‰å…¨å±€æè¿°è¡Œï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€è¡Œæè¿°ä½œä¸ºæ ‡é¢˜
                        if final_event_name == "Ã‰vÃ©nement" and global_description_lines:
                            final_event_name = global_description_lines[0]
                        
                        if main_instructor:
                            event.name = f"{final_event_name} - {main_instructor}"
                        else:
                            event.name = final_event_name
                        
                        # æ„å»ºæè¿°
                        desc_parts = []
                        if main_instructor and main_instructor not in event_name:
                            desc_parts.append(f"æ•™å¸ˆ: {main_instructor}")
                        
                        # æ·»åŠ å…¶ä»–æ—¶é—´æ®µä¿¡æ¯åˆ°æè¿°ä¸­ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ—¶é—´æ®µï¼‰
                        if len(time_events) > 1:
                            other_times = [f"{te['start_str']}-{te['end_str']} : {te['event_name']}" 
                                         for te in time_events if te != time_event]
                            if other_times:
                                desc_parts.extend(other_times)
                        
                        if global_description_lines:
                            desc_parts.extend(global_description_lines)
                        
                        event.description = "\n".join(desc_parts) if desc_parts else ""
                        
                        # æ·»åŠ æ—¶é—´æˆ³ (Phase 1: æ ‡å‡†å¿…éœ€å±æ€§)
                        event.created = datetime.now(tz=paris_tz)
                        event.last_modified = datetime.now(tz=paris_tz)
                        
                        events.append(event)
                        
                except (ValueError, IndexError) as e:
                    logger.warning(f"è§£æäº‹ä»¶æ—¶å‡ºé”™ '{time_line}': {e}")
        
        elif time_line:
            # å¤„ç†æ—§çš„å•æ—¶é—´äº‹ä»¶é€»è¾‘ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
            # æ£€æŸ¥è¿™ä¸ªäº‹ä»¶æ˜¯å¦è¢«éƒ¨åˆ†å–æ¶ˆ
            event_cancelled = False
            for cancelled_content in cancelled_content_list:
                if cancelled_content and (
                    cancelled_content in block or 
                    any(cancelled_content in line for line in lines) or
                    (start_str and end_str and f"{start_str}-{end_str}" in cancelled_content)
                ):
                    logger.info(f"âš ï¸  è·³è¿‡è¢«å–æ¶ˆçš„äº‹ä»¶: {time_line[:50]}...")
                    event_cancelled = True
                    break
            
            if event_cancelled:
                continue
                
            try:
                time_match = re.search(Config.TIME_PATTERN, time_line)
                start_str, end_str = time_match.groups()
                
                start_time = parse_time(start_str)
                end_time = parse_time(end_str)
                
                if start_time and end_time:
                    event = Event()
                    # ä½¿ç”¨å·´é»æ—¶åŒº
                    paris_tz = ZoneInfo(Config.TIMEZONE)
                    event.begin = datetime(year, month, day, start_time.hour, start_time.minute, tzinfo=paris_tz)
                    event.end = datetime(year, month, day, end_time.hour, end_time.minute, tzinfo=paris_tz)
                    
                    # æ„å»ºäº‹ä»¶æ ‡é¢˜
                    if instructor:
                        event.name = f"{event_name} - {instructor}"
                    else:
                        event.name = event_name
                    
                    # æ„å»ºæè¿°
                    desc_parts = []
                    if instructor and instructor not in event_name:
                        desc_parts.append(f"æ•™å¸ˆ: {instructor}")
                    if description_lines:
                        desc_parts.extend(description_lines)
                    
                    event.description = "\n".join(desc_parts) if desc_parts else ""
                    
                    # æ·»åŠ æ—¶é—´æˆ³ (Phase 1: æ ‡å‡†å¿…éœ€å±æ€§)
                    paris_tz = ZoneInfo(Config.TIMEZONE)
                    event.created = datetime.now(tz=paris_tz)
                    event.last_modified = datetime.now(tz=paris_tz)
                    
                    events.append(event)
                    
            except (ValueError, IndexError) as e:
                logger.warning(f"è§£æäº‹ä»¶æ—¶å‡ºé”™ '{time_line}': {e}")
        
        else:
            # å¤„ç†æ²¡æœ‰æ—¶é—´ä¿¡æ¯çš„äº‹ä»¶å— (ä¾‹å¦‚ "Projets collaboratifs")
            # æ£€æŸ¥è¿™ä¸ªäº‹ä»¶æ˜¯å¦è¢«å–æ¶ˆ
            event_cancelled = False
            for cancelled_content in cancelled_content_list:
                if cancelled_content and cancelled_content in block:
                    logger.info(f"âš ï¸  è·³è¿‡è¢«å–æ¶ˆçš„äº‹ä»¶: {block[:50]}...")
                    event_cancelled = True
                    break
            
            if not event_cancelled and block.strip():
                # åˆ›å»ºå…¨å¤©äº‹ä»¶
                event = Event()
                # ä½¿ç”¨naive datetimeé¿å…æ—¶åŒºè½¬æ¢å¯¼è‡´æ—¥æœŸåç§» (ä¾‹å¦‚ 00:00 Paris -> 23:00 UTC å‰ä¸€å¤©)
                event.begin = datetime(year, month, day)
                event.make_all_day()
                
                # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                lines = block.strip().split('\n')
                event.name = lines[0].strip()
                
                # å…¶ä½™ä½œä¸ºæè¿°
                if len(lines) > 1:
                    event.description = "\n".join(line.strip() for line in lines[1:])
                
                # æ·»åŠ æ—¶é—´æˆ³
                paris_tz = ZoneInfo(Config.TIMEZONE)
                event.created = datetime.now(tz=paris_tz)
                event.last_modified = datetime.now(tz=paris_tz)
                
                events.append(event)
    
    return events


class ScheduleProcessor:
    """è¯¾ç¨‹è®¡åˆ’å¤„ç†å™¨"""
    
    def __init__(self):
        self.calendar = Calendar()
        self.config_cancelled_dates = load_cancelled_dates()
        self._setup_calendar_properties()
    
    def _setup_calendar_properties(self):
        """è®¾ç½®æ—¥å†çš„Webcalè®¢é˜…å±æ€§"""
        # Phase 1: å®‰å…¨çš„æ ‡å‡†å±æ€§
        self.calendar.method = Config.CALENDAR_METHOD
        
        # ä¸åœ¨è¿™é‡Œè®¾ç½®extraå±æ€§ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
        # æ‰€æœ‰ç‰¹æ®Šå±æ€§éƒ½åœ¨ä¿å­˜æ—¶é€šè¿‡_inject_webcal_propertiesæ·»åŠ 
        
        logger.info(f"ğŸ“¡ é…ç½®Webcalè®¢é˜…: æ¯{Config.REFRESH_INTERVAL_HOURS}å°æ—¶æ›´æ–°")
        
    def process_schedule(self) -> bool:
        """
        å¤„ç†è¯¾ç¨‹è®¡åˆ’ï¼Œç”ŸæˆICSæ–‡ä»¶
        
        Returns:
            bool: æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        with timer("æ•´ä¸ªå¤„ç†è¿‡ç¨‹"):
            logger.info("ğŸ“… å¼€å§‹ç”Ÿæˆè¯¾ç¨‹æ—¥å† (V5.1 - æ·±åº¦ä¼˜åŒ–ç‰ˆæœ¬)...")
            
            # åŠ¨æ€è·å–æœˆä»½å’ŒURL
            with timer("æå–æœˆä»½URL"):
                month_urls = extract_month_urls()
                if not month_urls:
                    logger.error("æ— æ³•è·å–æœˆä»½ä¿¡æ¯ï¼Œç¨‹åºé€€å‡º")
                    return False
            
            # å¤„ç†æ¯ä¸ªæœˆä»½çš„æ•°æ®
            for month_name, csv_url in month_urls.items():
                with timer(f"å¤„ç† {month_name} æ•°æ®"):
                    if not self._process_month_data(month_name, csv_url):
                        logger.warning(f"å¤„ç† {month_name} æ•°æ®å¤±è´¥ï¼Œè·³è¿‡...")
                        continue
            
            # ç”Ÿæˆæœ€ç»ˆçš„ICSæ–‡ä»¶
            with timer("ä¿å­˜ICSæ–‡ä»¶"):
                return self._save_calendar()
    
    def _process_month_data(self, month_name: str, csv_url: str) -> bool:
        """å¤„ç†å•ä¸ªæœˆä»½çš„æ•°æ®"""
        logger.info(f"æ­£åœ¨å¤„ç† {month_name.capitalize()} çš„æ•°æ®...")
        
        try:
            # HTMLåˆ é™¤çº¿æ£€æµ‹
            html_cancelled_dates, html_cancelled_events = detect_strikethrough_from_html(csv_url)
            all_cancelled_dates = self.config_cancelled_dates.union(html_cancelled_dates)
            
            # ä¸‹è½½CSVæ•°æ®
            csv_data = self._download_csv_data(csv_url)
            if not csv_data:
                return False
                
            # è§£æCSVæ•°æ®å¹¶æ·»åŠ äº‹ä»¶
            self._parse_csv_data(csv_data, all_cancelled_dates, html_cancelled_dates, html_cancelled_events)
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç† {month_name} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _download_csv_data(self, csv_url: str) -> Optional[List[List[str]]]:
        """ä¸‹è½½å¹¶è§£æCSVæ•°æ®"""
        try:
            response = requests.get(csv_url, headers=Config.HEADERS, timeout=Config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            csv_data_string = response.content.decode('utf-8')
            csv_file = StringIO(csv_data_string)
            reader = csv.reader(csv_file)
            data = list(reader)
            
            if not data or len(data) < 2:
                logger.warning("CSVæ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
                return None
                
            return data
            
        except requests.RequestException as e:
            logger.error(f"ä¸‹è½½CSVæ•°æ®å¤±è´¥: {e}")
            return None
        except UnicodeDecodeError as e:
            logger.error(f"CSVç¼–ç è§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"è§£æCSVæ•°æ®å¤±è´¥: {e}")
            return None
    
    def _parse_csv_data(self, data: List[List[str]], all_cancelled_dates: Set[Tuple[int, int, int]], 
                       html_cancelled_dates: Set[Tuple[int, int, int]], 
                       html_cancelled_events: List[CancelledEvent]) -> None:
        """è§£æCSVæ•°æ®å¹¶æ·»åŠ äº‹ä»¶åˆ°æ—¥å†"""
        i = 0
        while i < len(data):
            row = data[i]
            
            # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦åŒ…å«æ—¥æœŸ
            if self._row_contains_date(row):
                date_row = row
                
                # å¤„ç†ä¸‹ä¸€è¡Œçš„è¯¾ç¨‹å†…å®¹
                if i + 1 < len(data):
                    content_row = data[i + 1]
                    self._process_date_content_pair(
                        date_row, content_row, all_cancelled_dates, 
                        html_cancelled_dates, html_cancelled_events
                    )
                
                i += 2  # è·³è¿‡å†…å®¹è¡Œ
            else:
                i += 1
    
    def _row_contains_date(self, row: List[str]) -> bool:
        """æ£€æŸ¥è¡Œæ˜¯å¦åŒ…å«æ—¥æœŸ"""
        return any(re.search(r'\d{1,2}/\d{1,2}', cell) for cell in row)
    
    def _process_date_content_pair(self, date_row: List[str], content_row: List[str],
                                  all_cancelled_dates: Set[Tuple[int, int, int]],
                                  html_cancelled_dates: Set[Tuple[int, int, int]],
                                  html_cancelled_events: List[CancelledEvent]) -> None:
        """å¤„ç†æ—¥æœŸè¡Œå’Œå†…å®¹è¡Œçš„é…å¯¹"""
        for date_cell, content_cell in zip(date_row, content_row):
            if not content_cell.strip():
                continue
                
            parsed_date = self._extract_date_from_cell(date_cell)
            if not parsed_date:
                continue
                
            year, month, day = parsed_date
            
            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if (year, month, day) in all_cancelled_dates:
                source = "HTMLåˆ é™¤çº¿æ£€æµ‹" if (year, month, day) in html_cancelled_dates else "é…ç½®æ–‡ä»¶"
                logger.info(f"è·³è¿‡ {day:02d}/{month:02d}/{year} çš„è¯¾ç¨‹ ({source})")
                continue
            
            logger.info(f"å¤„ç† {day:02d}/{month:02d}/{year} çš„è¯¾ç¨‹...")
            
            # è§£æå•å…ƒæ ¼å†…å®¹å¹¶æ·»åŠ äº‹ä»¶
            events = parse_cell_content(content_cell, year, month, day, html_cancelled_events)
            for event in events:
                self.calendar.events.add(event)
                logger.info(f"æ·»åŠ äº‹ä»¶: {event.name}")
    
    def _extract_date_from_cell(self, cell: str) -> Optional[Tuple[int, int, int]]:
        """ä»å•å…ƒæ ¼æå–æ—¥æœŸä¿¡æ¯"""
        date_match = re.search(r'(\d{1,2})/(\d{1,2})', cell)
        if date_match:
            day, month = int(date_match.group(1)), int(date_match.group(2))
            # 1æœˆåˆ°7æœˆå±äº2026å¹´ï¼ˆæ˜¥å­£å­¦æœŸï¼‰ï¼Œ8æœˆåˆ°12æœˆå±äº2025å¹´ï¼ˆç§‹å­£å­¦æœŸï¼‰
            year = 2026 if 1 <= month <= 7 else 2025
            return (year, month, day)
        return None
    
    def _save_calendar(self) -> bool:
        """ä¿å­˜æ—¥å†åˆ°æ–‡ä»¶ï¼ŒåŒ…å«Webcalè®¢é˜…å±æ€§"""
        try:
            # è·å–åŸå§‹çš„ICSå†…å®¹ï¼ˆä½¿ç”¨serializeæ–¹æ³•é¿å…cloneé”™è¯¯ï¼‰
            try:
                # æ¨èæ–¹æ³•ï¼šä½¿ç”¨serialize()
                ics_content = self.calendar.serialize()
            except AttributeError:
                # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨serialize_iter()
                try:
                    ics_content = ''.join(self.calendar.serialize_iter())
                except AttributeError:
                    # æœ€åå¤‡ç”¨ï¼šä½¿ç”¨str()ä½†ä¼šæœ‰è­¦å‘Š
                    import warnings
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        ics_content = str(self.calendar)
            
            # æ‰‹åŠ¨æ³¨å…¥Webcalè®¢é˜…å±æ€§
            ics_content = self._inject_webcal_properties(ics_content)
            
            with open(Config.OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
                f.write(ics_content)
            
            logger.info(f"ğŸ‰ æˆåŠŸ! æ—¥å†æ–‡ä»¶å·²ç”Ÿæˆ: {os.path.abspath(Config.OUTPUT_FILENAME)}")
            logger.info("è¯·å°†æ­¤æ–‡ä»¶å¯¼å…¥æ‚¨çš„iPhoneæ—¥å†ã€‚")
            logger.info(f"æ€»å…±ç”Ÿæˆäº† {len(self.calendar.events)} ä¸ªäº‹ä»¶")
            
            # æ˜¾ç¤ºWebcalé…ç½®ä¿¡æ¯
            if Config.ENABLE_REFRESH_INTERVAL:
                logger.info(f"ğŸ“¡ å·²é…ç½®Webcalè®¢é˜…: æ¯{Config.REFRESH_INTERVAL_HOURS}å°æ—¶æ›´æ–°")
            
            return True
            
        except PermissionError:
            logger.error(f"æ²¡æœ‰æƒé™å†™å…¥æ–‡ä»¶: {Config.OUTPUT_FILENAME}")
            return False
        except Exception as e:
            logger.error(f"ä¿å­˜æ—¥å†æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _inject_webcal_properties(self, ics_content: str) -> str:
        """åœ¨ICSå†…å®¹ä¸­æ³¨å…¥Webcalè®¢é˜…å±æ€§"""
        try:
            lines = ics_content.split('\n')
            injected_lines = []
            
            for line in lines:
                injected_lines.append(line)
                
                # åœ¨VCALENDARå¼€å§‹åç«‹å³æ·»åŠ å±æ€§
                if line.strip() == 'BEGIN:VCALENDAR':
                    # Phase 2: æ ‡å‡†æ›´æ–°æ§åˆ¶
                    if Config.ENABLE_REFRESH_INTERVAL:
                        refresh_interval = f"PT{Config.REFRESH_INTERVAL_HOURS}H"
                        injected_lines.append(f"REFRESH-INTERVAL:{refresh_interval}")
                    
                    # Phase 3: æ‰©å±•å±æ€§
                    if Config.PUBLISH_TTL_HOURS:
                        ttl_interval = f"PT{Config.PUBLISH_TTL_HOURS}H"
                        injected_lines.append(f"X-PUBLISHED-TTL:{ttl_interval}")
                    
                    # æ·»åŠ æ—¥å†æè¿°ä¿¡æ¯
                    injected_lines.append(f"X-WR-CALDESC:MIASHSè¯¾ç¨‹è®¡åˆ’ - æ¯{Config.REFRESH_INTERVAL_HOURS}å°æ—¶æ›´æ–°")
                    injected_lines.append(f"X-WR-CALNAME:MIASHS Master Handicap Schedule")
            
            return '\n'.join(injected_lines)
            
        except Exception as e:
            logger.error(f"æ³¨å…¥Webcalå±æ€§å¤±è´¥: {e}")
            # è¿”å›åŸå§‹å†…å®¹ï¼Œè‡³å°‘ä¿è¯åŸºæœ¬åŠŸèƒ½
            return ics_content


# ä¿æŒå…¼å®¹æ€§çš„ç‹¬ç«‹å‡½æ•°
def parse_schedule_from_dynamic_urls():
    """æ—§ç‰ˆæœ¬å…¼å®¹å‡½æ•°"""
    main()
def main() -> None:
    """ä¸»å‡½æ•°"""
    try:
        processor = ScheduleProcessor()
        success = processor.process_schedule()
        if not success:
            logger.error("ç¨‹åºæ‰§è¡Œå¤±è´¥")
            exit(1)
        else:
            logger.info("ç¨‹åºæ‰§è¡ŒæˆåŠŸå®Œæˆ!")
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºå‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        exit(1)


if __name__ == "__main__":
    main()